"""
Author: Kole Phillips

Aggregate the existing table of power generated by the top_generation.py script
to the specified time interval. If no time interval is provided, the default
(1 min) is used instead.
Also computes the activity levels from the summary counts column, as activity
levels are not present in the 1 second version of the table of power.

Usage: python top_generation.py CITY WAVE DIRECTORY
  CITY: The name of the city the table is to represent
  WAVE: The wave number of the study to represent in the table
  DIRECTORY: The location of the existing table of power csv, as well as where the new file will be stored
"""

import interact_tools as it
import pandas as pd
import geopandas as gpd
from os.path import isfile, isdir
from sys import argv
import top_generation as tg
import numpy as np
from wear_time import wear_marking


if __name__ == "__main__":
    city, wave = it.get_command_args("top_minute.py")
    if '-e' in argv:
        ethica = True
    else:
        ethica = False
    if len(argv) < 4:
        print("Usage: python top_minute.py SITE_NAME WAVE_NUMBER DIRECTORY")
        exit()
    output_dir = argv[3]
    if not isdir(output_dir):
        print("Could not locate directory: " + output_dir)
    if len(argv) > 5 and argv[5] in ['S', 'min', 'T', 'H', 'D'] and argv[4].isdigit():
        time_type = argv[5]
        time_quant = argv[4]
    else:
        print("Using default (1min) time interval.")
        time_type = "min"
        if ethica:
            time_quant = "5"
        else:
            time_quant = "1"

    ver = it.get_last_commit_date()
    if wave < 10:
        top_fname = output_dir + '/' + city + '_0' + str(wave) + '_table_of_power_' + ver
        out_fname = output_dir + '/' + city + '_0' + str(wave) + '_top_' + time_quant + time_type + '_' + ver
    else:
        top_fname = output_dir + '/' + city + '_' + str(wave) + '_table_of_power_' + ver
        out_fname = output_dir + '/' + city + '_' + str(wave) + '_top_' + time_quant + time_type + '_' + ver
    if ethica:
        top_fname = top_fname + '_ethica' + '.csv'
        out_fname = out_fname + '_ethica' + '.csv'
    else:
        top_fname = top_fname + '_sd' + '.csv'
        out_fname = out_fname + '_sd' + '.csv'

    if not isfile(top_fname):
        print("Could not locate file: " + top_fname)


    chunk_size = 10 ** 6
    header = True

    processed_min = []
    if isfile(out_fname):
        header = False
        data_chunks = pd.read_csv(out_fname, chunksize=chunk_size)
        for chunk in data_chunks:
            processed_min = processed_min + chunk['interact_id'].drop_duplicates().to_list()
        processed_min = list(set(processed_min))

    light_thresh = 100
    moderate_thresh = 2020
    vigorous_thresh = 5999

    if int(time_quant) > 1:
        light_thresh = light_thresh * int(time_quant)
        moderate_thresh = moderate_thresh * int(time_quant)
        vigorous_thresh = vigorous_thresh * int(time_quant)
    if time_type == "S":
        light_thresh = light_thresh / 60
        moderate_thresh = moderate_thresh / 60
        vigorous_thresh = vigorous_thresh / 60
    if time_type == "H":
        light_thresh = light_thresh * 60
        moderate_thresh = moderate_thresh * 60
        vigorous_thresh = vigorous_thresh * 60
    if time_type == "D":
        light_thresh = light_thresh * 60 * 24
        moderate_thresh = moderate_thresh * 60 * 24
        vigorous_thresh = vigorous_thresh * 60 * 24
    if ethica:
        light_thresh = light_thresh / 5
        moderate_thresh = moderate_thresh / 5
        vigorous_thresh = vigorous_thresh / 5

    # loading the .shp file for the specified city (only Victoria for now)
    sf = gpd.GeoDataFrame.from_file("CMA/INTERACT_CMA_EPSG4326.shp")
    processed = []
    data_chunks = pd.read_csv(top_fname, chunksize=chunk_size)

    for chunk in data_chunks:
        processed = processed + chunk['interact_id'].drop_duplicates().to_list()
    processed = list(set(processed))

    for p in processed:
        print("Processing participant " + str(p) + ".")
        if p in processed_min:
            print("Already processed")
            continue
        data_chunks = pd.read_csv(top_fname, chunksize=chunk_size)
        p_data = pd.DataFrame()
        for chunk in data_chunks:
            if p_data.empty:
                p_data = chunk.loc[chunk['interact_id'] == p]
            else:
                p_data = p_data.append(chunk.loc[chunk['interact_id'] == p])

        p_data['utcdate'] = pd.to_datetime(p_data.utcdate)
        p_data.set_index('utcdate', inplace=True, drop=True)

        mean_data = p_data.resample(time_quant + time_type).mean().dropna(subset=['interact_id'])
        static_data = p_data.resample(time_quant + time_type).first().dropna(subset=['interact_id'])
        sum_data = p_data.resample(time_quant + time_type).sum().dropna(subset=['interact_id'])
        static_data[['interact_id', 'age', 'wave_id']] = static_data[['interact_id', 'age', 'wave_id']].astype(int)
        # aggregate the existing data depending on its data type
        m_cols = ['lon', 'lat', 'northing', 'easting']
        s_cols = ['x_count', 'y_count', 'z_count', 'summary_count']
        f_cols = ['interact_id', 'zone', 'age', 'gender', 'income', 'education', 'ethnicity', 'city_id', 'wave_id']
        minute_data = p_data.drop(['in_city', 'wearing'], axis=1)[m_cols].resample(time_quant + time_type).median()
        minute_data[f_cols] = p_data.drop(['in_city', 'wearing'], axis=1)[f_cols].resample(time_quant + time_type).first()
        minute_data[s_cols] = p_data.drop(['in_city', 'wearing'], axis=1)[s_cols].resample(time_quant + time_type).sum()
        minute_data = minute_data.dropna(subset=['interact_id'])
        """minute_data = p_data.drop(['in_city', 'wearing'], axis=1).resample(time_quant + time_type).agg({
            'lon': np.median,
            'lat': np.median,
            'northing': np.median,
            'easting': np.median,
            'interact_id': 'first',
            'zone': 'first',
            'x_count': np.sum,
            'y_count': np.sum,
            'z_count': np.sum,
            'summary_count': np.sum,
            'age': 'first',
            'gender': 'first',
            'income': 'first',
            'education': 'first',
            'city_id': 'first',
            'wave_id': 'first',
            })"""
        # }).dropna(subset=['summary_count'])
        # in_city and wearing columns are present in the seconds version, but must be recomputed entirely
        minute_data['in_city'] = tg.in_city(minute_data, sf, city)
        minute_data['wearing'] = wear_marking(p_data[['summary_count']], epoch=time_quant + time_type)['wearing']

        minute_data['activity_levels'] = "vigorous"
        minute_data.loc[minute_data.x_count < vigorous_thresh, 'activity_levels'] = "moderate"
        minute_data.loc[minute_data.x_count < moderate_thresh, 'activity_levels'] = "light"
        minute_data.loc[minute_data.x_count < light_thresh, 'activity_levels'] = "sedentary"
        minute_data = minute_data.reset_index()[["interact_id", "utcdate", "lon", "lat", "northing",
                                                 "easting", "zone", "in_city", "x_count", "y_count", "z_count",
                                                 "summary_count", "wearing", "activity_levels", "age", "gender",
                                                 "income", "ethnicity", "city_id", "wave_id"]]
        #minute_data = minute_data.reset_index()[["interact_id", "utcdate", "lon", "lat", "northing",
        #                                         "easting", "zone", "in_city", "x_count", "y_count", "z_count",
        #                                         "summary_count", "wearing", "activity_levels", "age", "gender",
        #                                         "income", "education", "ethnicity", "city_id", "wave_id"]]
        minute_data[["interact_id", "in_city", "x_count", "y_count", "z_count", "wearing", "age", "wave_id"]] = \
            minute_data[["interact_id", "in_city", "x_count", "y_count", "z_count", "wearing", "age", "wave_id"]].astype(int)
        minute_data = minute_data.set_index(['interact_id', 'utcdate'])

        if header:
            # Create a new csv file if this is the first participant processed
            minute_data.to_csv(out_fname)
            header = False
        else:
            # Otherwise, append this participant's data to the end of the file
            minute_data.to_csv(out_fname, header=False, mode='a')

